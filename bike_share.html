<!DOCTYPE html>
<html>
<style>
html {
    -webkit-font-smoothing: antialiased; /*To remove blurry text*/
  }
body {
  background-size: 900px auto;
  background-repeat: no-repeat;
  background-position: left; 
  background-attachment: fixed;
  font-family: verdana !important;
  font-size: 14px !important;
}
h1, h2, h3, h4, h5, h6, p {
  font-variant: small-caps !important;  
  text-rendering: auto;
  vertical-align: middle;
  font-family: verdana;
}
nav a {
  color: #eee;
  font-variant: small-caps;
}
a.page {
  color: steelblue;
}
a:hover, a:active, a:focus {
  color: #222 !important;
  background-color: #eee;
  text-decoration: none !important;
}
/*--------------CSS------------------------NAVBAR CSS---------------------------------------------------------*/
#navLeft, #navRight {
  width: 50%;
  float: left;
  margin: 0 0 0 0;
  padding: 0;
  background-color: #313232;
}
#navLeft ul, #navRight ul {
  list-style: none;
  margin: 0;
  padding: 0; 
}
#navLeft li, #navRight li {
  float: right;
  display: inline;
}
#navLeft li {
  float: left;
  display: inline;
}
#navLeft li a, #navRight li a {
  display: block;
  padding: 8px 15px;
  text-decoration: none;
}
/*-------------------------------------------------------------------*/
/*#Main_section {
  margin-top: 60px;
}*/
.Story1::first-letter {
  color: #222;
  font-size: xx-large;
  background-color: #eee;
  padding-right: 5px;
  padding-left: 5px;
  margin-left: -5px;
  margin-right: 5px;
}
.imageContainers {
  background-color: #eee;
}
</style>

<head>
    <title>Bike Share Rental Prediction</title>
    <link rel="stylesheet" href = "//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/darkly/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- jQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</head>

<nav class="navContainer navbar-fixed-top" role="navigation">
  <div id="navLeft">
    <ul>
      <li><a href="index">Data Science Online Master</a></li>
      <li><a href="network">Detailed Exploration</a></li>
      <li><a href="statistics">Statistical Analysis</a></li>
    </ul>
  </div>
  <div id="navRight">
    <ul>
      <li class="right"><a href="contact">Contact Us</a></li>
      <li><a href="acknowledgment">Special Thanks</a></li>
    </ul>
  </div>
</nav>


<body>
  <div id="Main_section">
    <div class="row headline1">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
        <h3 class="text-center">Bike Sharing Demand</h3>
        <div class="text-justify Story1">
        This problem was hosted by Kaggle as a knowledge competition and was an opportunity to practice a regression problem on an easily manipulatable dataset. The aim was to predict as accurately as possible bike rentals for the 20th day of the month by using the bike rentals from the previous 19 days that month, using two year's worth of data.
        <div><br><ul style="list-style-type:disc">
          <li>Models were scored on their ability to minimise the <a class="page" href='https://www.kaggle.com/wiki/LogarithmicLoss'> Root Mean Squared Logarithmic Error </a> function</li>
          <li>The benchmark to beat in this competition was <b>1.58456</b></li>
          <li>Our linear regression model had an error of <b>0.65049</b></li>
          <li>Our Extremely Random Forest model had an error of <b>0.48668</b></li>
        </ul></div>
        <h4><small><font color='#909090'> We would like to thank <a class="page" href='https://www.kaggle.com/'> Kaggle </a> for another great competition</font></small></h4>
        Please refer to the accompanying <a class="page" href="http://nbviewer.ipython.org/github/fraser-campbell/Machine-Learning-Projects/blob/master/Machine-Learning-Projects/Avazu/Avazu%20Click%20Through%20Rate.ipynb">ipython notebook</a> for details of the analysis we undertook. A summary is documented below.

      <h2 class="text-left"><small>Exploratory Analysis</small></h2>
      We began by conducting exploratory analysis on the bike rental data. We were provided with bike rentals per hour (shared across 'casual' and 'registered' users) along with a number of features associated with that hour. A number of trends were immediately apparent and suggested that certain features would be strong predictors.
      <br><br>
      We observe that 'registered' users show a strong bimodal distribution with hour of the day, as they probably use their bike to travel to and from work. On the hand, this is not observed for the 'casual' users whose usage peaks in the middle of the day. Also we can see an increase in bike usage from the 'casual' users at the weekend compared to the week and an oppositive behaviour from the 'registered' users.
      <br><br>
      </div>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-1 buffer"></div>
      <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="bike1.png" width = "1000" height= "auto"></div>
      </div>
      <div class="col-md-1 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 1: Average Bike Rental Count vs Hours</font></h5>
      <br>
      The weather also plays a significant factor in rentals, with temperature being particularly significant:
      <br><br>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-1 buffer"></div>
      <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="bike2.png" width = "1000" height= "auto"></div>
      </div>
      <div class="col-md-1 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 2: Average Bike Rental Count vs Weather Condition</font></h5>

      <h2 class="text-left"><small>Linear Regression</small></h2>
      Initially we ran a standard linear regression model, however its predicitive power was drastically limited for a number of reasons:
      <div><br><ul style="list-style-type:disc">
        <li>Count dataset composed of exclusively positive numbers</li>
        <li>Highly non-linear behaviour and heteroskedacity seen in the residuals violating linear regression assumptions</li>
      </ul></div>
      <br>
      Below is a plot of the linear model residuals:
      <br>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <div class ="imageContainers" align="middle"><img src="bike3.png" width = "600" height= "auto"></div>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 3: Bike Rental Residuals</font></h5>
      <br>
      To begin addressing the above, we took the log of the count data and built a linear regression model. By doing so, we can ensure the count values returned from our model are always positive.
      <br><br>
      The following is a plot of our predicted values against actual values tested on our held-out dataset, using the Log-Linear model:
      <br>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <div class ="imageContainers" align="middle"><img src="bike4.png" width = "600" height= "auto"></div>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 4: Predicted Rental vs Actual Rental - Linear Regression</font></h5>
      <br>
      This model after submission to Kaggle generated a Root Mean Squared Logarithmic Error of <b>0.65049</b>. A respectable start.


      <h2 class="text-left"><small>Random Forest</small></h2>
      We now implemented a Random Forest and Extremely Random Forest model, both of which are non parametric models which can be used for both regression and classification. The basic premise of these models is to create a very large number of random decision trees using our training dataset and then averaging over these trees, which has the effect of enabling the signal to rise above the noise. Random Forests, especially when trained on a large number of trees, have proven to provide good predictions and can model features of both continous and categorical types.
      <br><br>
      We undertook the following steps to optimise and test both our Random Forest and Extremely Random Forest models:
      <div><br><ul style="list-style-type:disc">
        <li>1. Hyperparameter selection through K-Fold cross validation</li>
        <li>2. Optimal model fitting</li>
        <li>3. Feature engineering and model combining</li>
      </ul></div>
      <br> The first two of these enabled us to fit a significantly improved model with a Root Mean Squared Logarithmic Error of <b>0.48668</b>.
      Step 3 consisted of undertaking PCA on the data set to pre-process it prior to fitting the Random Forest as well as creating two separate models for the 'registered' and 'casual' users, to better honour their differences. However, frustratingly, both feature engineering and model combining were unable to improve our score.
      <br><br>
      Neverthless our Extra random forest did succeed in generating the following predictions relative to the test set <em>(note the decreased dispersion at high counts)</em>:
      <br>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <div class ="imageContainers" align="middle"><img src="bike5.png" width = "600" height= "auto"></div>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 5: Predicted Rental vs Actual Rental - Extremely Random Forest</font></h5>
      <h2 class="text-left"><small>Next Steps</small></h2>
      The main avenue for model improvement seems to be feature creation or feature extraction. However so far, this has not yielded fruitful results. We will leave you with a plot of the residuals against the different features, which highlights the feature values we are struggling to predict.<br>
      Maybe you have some ideas?
      <br><br>
      </div>
      <div class="col-md-2 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-1 buffer"></div>
      <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="bike6.png" width = "1000" height= "auto"></div>
      </div>
      <div class="col-md-1 buffer"></div>
    </div>
    <div class="row">
      <div class="col-md-2 buffer"></div>
      <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 6: Current Optimal Model Residuals vs Features</font></h5>

      <h4><small><font color='#909090'> The competition will run on <a class="page" href='https://www.kaggle.com/'> Kaggle </a> until Fri 29th May 2015, at which stage we can all learn from the best solutions</font></small></h4>
      <br><br>

      </div>
    </div>
  </div>

<script type="text/javascript">
  //Ensure body always stays below the nav-bar
  $(window).resize(function () { 
    $('body').css('padding-top', parseInt($('#navLeft').css("height")));
  });

  $(window).load(function () { 
      $('body').css('padding-top', parseInt($('#navLeft').css("height")));        
  });
</script>

</body>

</html>