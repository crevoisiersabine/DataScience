<!DOCTYPE html>
<html>
<style>
html {
    -webkit-font-smoothing: antialiased; /*To remove blurry text*/
  }
body {
  background-size: 900px auto;
  background-repeat: no-repeat;
  background-position: left; 
  background-attachment: fixed;
  font-family: verdana !important;
  font-size: 14px !important;
}
h1, h2, h3, h4, h5, h6, p {
  font-variant: small-caps !important;  
  text-rendering: auto;
  vertical-align: middle;
  font-family: verdana;
}
nav a {
  color: #eee;
  font-variant: small-caps;
}
a.page {
  color: steelblue;
}
a:hover, a:active, a:focus {
  color: #222 !important;
  background-color: #eee;
  text-decoration: none !important;
}
/*--------------CSS------------------------NAVBAR CSS---------------------------------------------------------*/
#navLeft, #navRight {
  width: 50%;
  float: left;
  margin: 0 0 0 0;
  padding: 0;
  background-color: #313232;
}
#navLeft ul, #navRight ul {
  list-style: none;
  margin: 0;
  padding: 0; 
}
#navLeft li, #navRight li {
  float: right;
  display: inline;
}
#navLeft li {
  float: left;
  display: inline;
}
#navLeft li a, #navRight li a {
  display: block;
  padding: 8px 15px;
  text-decoration: none;
}
/*-------------------------------------------------------------------*/
/*#Main_section {
  margin-top: 60px;
}*/
.Story1::first-letter {
  color: #222;
  font-size: xx-large;
  background-color: #eee;
  padding-right: 5px;
  padding-left: 5px;
  margin-left: -5px;
  margin-right: 5px;
}
.imageContainers {
  background-color: #eee;
}
</style>

<head>
    <title>Data Science Statistics</title>
    <link rel="stylesheet" href = "//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/darkly/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- jQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</head>

<body>

<nav class="navContainer navbar-fixed-top" role="navigation">
  <div id="navLeft">
    <ul>
      <li><a href="index">Data Science Online Master</a></li>
      <li><a href="network">Detailed Exploration</a></li>
      <li><a href="statistics">Statistical Analysis</a></li>
    </ul>
  </div>
  <div id="navRight">
    <ul>
      <li class="right"><a href="contact">Contact Us</a></li>
      <li><a href="acknowledgment">Special Thanks</a></li>
    </ul>
  </div>
</nav>

<div id="Main_section">
  <div class="row headline1">
    <div class="col-md-2 buffer"></div>
    <div class="col-md-8 buffer">
      <h3 class="text-center">Avazu Click Through Rate Competition</h3><br>
      <div class="text-justify Story1">
      This was our first Kaggle competition and first application of machine learning to what might be considered a 'Big Data' problem, with ~47 million training examples. The competition supplied 11 days of Avazu online advertising data in order to predict click through rates.
      <div><br><ul style="list-style-type:disc">
        <li>Models were scored on their ability to minimise the <a class="page" href='https://www.kaggle.com/wiki/LogarithmicLoss'> Log Loss </a> function on a held-out test dataset</li>
        <li>Our initial model submission had an average log loss on the test dataset of <b>0.5641370</b></li>
        <li>After tuning we obtained a log loss error of ?????????</li>
      </ul></div>
      <h4><small><font color='#909090'>We would like to thank the <a class="page" href='https://github.com/JohnLangford/vowpal_wabbit/wiki'> Vowpal Wabbit project </a> for an amazing software and <a class="page" href='https://www.kaggle.com/'> Kaggle </a> for creating their data science community</font></small></h4>

      <h2 class="text-left"><small>Big Data Challenges</small></h2>
      This dataset was too large to be read into computer memory and complex modelling on it was proving to very computationally expensive. In the exploratory analysis below, we discuss the issues in working within ipython notebook, and a more detailed version, as well as the code, can be accessed on the <a class="page" href="http://nbviewer.ipython.org/github/fraser-campbell/Machine-Learning-Projects/blob/master/Machine-Learning-Projects/Avazu/Avazu%20Click%20Through%20Rate.ipynb">ipython notebook</a> itself.<br>
      Due to the difficulty that we experienced with loading the dataset, our only option for working on our local machine appeared to be statistically modelling a sub-sample of the dataset.
      <br><br>
      This led us to consider three alternative options:
      <div><br><ul style="list-style-type:disc">
        <li>Sampling and Statistical Analysis</li>
        <li>Online Learners <em>(Chosen)</em></li>
        <li>Parallelisation</li>
      </ul></div>

      <h2 class="text-left"><small>Exploratory Analysis</small></h2>
      Reading the data in by line to ipython notebook took ~ 1 min for 5000 lines. However from these inital lines we were able to explore the form of the data. Plotted below is a histogram of the unique values for each category, alongside a line chart of the percentage of clicks for each unique value.<br><br>
      This seeks to explore:<br>
      <li>How many features within each category?</li>
      <li>Do any features seem to have patterns of click / non-click?</li>
      <br>
      </div>
    </div>
    <div class="col-md-2 buffer"></div>
  </div>
  <div class="row">
    <div class="col-md-1 buffer"></div>
    <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="vw1.png" width = "1000" height= "auto"></div>
    </div>
    <div class="col-md-1 buffer"></div>
  </div>
  <div class="row">
    <div class="col-md-2 buffer"></div>
    <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 1: Category Exploration of Features</font></h5>
      <br>
      Some categories appear to have a very high number of unique values, which suggests the values may be specific to a small number of training examples and could be a candidate for removal to reduce features, as they do not include generalisable information.<br><br>In terms of click distribution, the 1000 row data sample is not very informative about the full dataset; however there appears to be some interesting traits, for example we can observe that in category C1, type "0", although not many examples are present, those available seem to consistently lead to a click event; this suggests that the feature C1-0 might be a good predictor of clicking.<br><br>In order to potentially reduce the number of features we then explored the growth of the feature space on considering incrementally more data to answer the following question.
      <li>Do some features scale approximately with the dataset?</li>
      <br>
    </div>
  </div> 
  <div class="row">
    <div class="col-md-1 buffer"></div>
    <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="vw2.png" width = "1000" height= "auto"></div>
    </div>
    <div class="col-md-1 buffer"></div>
  </div>
  <div class="row">
    <div class="col-md-2 buffer"></div>
    <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 2: Growth of Features</font></h5>
      From the above plots we can see that a few of the categories seem to scale with the size of the dataset. Particularly "device_id" and "device_ip" grow linearly with the number of examples and therefore decided to remove tehm from our features when training the model.
      <br><br>
      At this stage we experimented with fitting Naive Bayes and Logistic Regression Models to 5000 example subsets of the data. The details can be found in the <a class="page" href='http://nbviewer.ipython.org/github/fraser-campbell/Machine-Learning-Projects/blob/master/Machine-Learning-Projects/Avazu/Avazu%20Click%20Through%20Rate.ipynb'> python notebook </a> but in summary, as would be expected, the model was massivey overfit due to using too many features for the number of training examples. Both models ended up predicting exclusively no clicks as this maximised the accuracy. To proceed, we needed to consider an online learner that could handle the data size.

      </br><h2 class="text-left"><small>Vowpal Wabbit Online Learner</small></h2>
      <a class="page" href="http://hunch.net/~vw/">Vowpal Wabbit </a> is a large scale online learner, designed for learning from large datsets as it only reads a fraction of training examples into memory at any point. It seems highly suited for this example as it can accomodate models becoming out-dated, dealing with divergence of distributions over time and unbounded features (you may constantly collect new categorical feature values). We found the <a class="page" href='http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/'> ML Wave blog </a> to be a great introduction to using Vowpal Wabbit in Kaggle competitions.
      <br>
      We initially ran a Vowpal Wabbit logistic regression model. We were able to specfy the loss function to be log loss (--logistic loss) and achieved a log loss on the test sample upon submission to Kaggle of 0.5641370.
      (Note smaller is better).

      </br><h2 class="text-left"><small>Model Tuning & Debugging</small></h2>
      Vowpal Wabbit has the option to output the average loss to terminal while training, as an aide to debugging. In our above model the loss was seen to decrease with training as expected, but it had a 'bump' after which it increased. This could be due to the model overtraining to some aspect of the data which is more prevalent in the training examples initially, or perhaps the model learning rate. With regards to the first possibility, we postulated that a cause could be seasonality in the data and training to a particular day of the week and therefore tried to randomly shuffle the dataset.
      <br><br>
      Due to the file size, shuffling locally was not succesfull and neither was an attempted AWS MAP Reduce job on the dataset, due to file upload limitations. We therefore opted for bootstrapping <em> --bs </em>: taking the average parameters from 10 seperate models, this hopefully helps our final model to generalise. We also chose to run multiple passes over the data, hoping to avoid overfitting our model to one portion of the dataset. We used the option <em> --early_terminate</em> to stop the passes early if we observed the validation error consistently increases and therefore 'overfitting' occurs.
      <br><br>
      The plot below shows the log loss error with our initial model and our model using boostrapping and multiple passes.<br><br>
      <div class ="imageContainers" align="middle"><img src="vw3.png" width = "600" height= "auto"></div>
      <h5 align="middle"><font color='#909090'>Figure 3: Log loss versus number of training examples</font></h5>
      <br>
      The above plot shows that even with bootstrapping and running multiple passes, we observe the 'bump' showing increase in log loss error before stabilisation; this seems to indicate that the data would still benefit from being shuffled and probably suffers from some seasonality effects. However the 'bump' has been reduced so there seems to have been some advantage to running bootstrap and multiple passes.
      <br><br><br>
      We also considered whether feature reduction might be possible to lower computation time and potentially allow for more complex learning methods. Using the <em>vw--varinfo</em> option in Vowpal Wabbit we analysed the weightings given to different features within the trained models. We plotted histograms of feature weightings in order to explore the question:
      <li>How are the model features weighted within each category</li>
      <br>
    </div>
    <div class="col-md-2 buffer"></div>
  </div>
  <div class="row">
    <div class="col-md-1 buffer"></div>
    <div class="col-md-10 buffer">
      <div class ="imageContainers" align="middle"><img src="vw4.png" width = "1000" height= "auto"></div>
    </div>
    <div class="col-md-1 buffer"></div>
  </div>
  <div class="row">
    <div class="col-md-2 buffer"></div>
    <div class="col-md-8 buffer">
      <h5 align="middle"><font color='#909090'>Figure 3: Feature Weighting</font></h5>

      <br>We also explored a number of options to tune the parameters to reduce the log loss. We used the <em> vw-hypersearch </em> command to train the model on a range of hyperparameters and record how the validation error changes. We consider both the learning rate and l1 regularisation. The validation error is not seen to change significantly which suggests that the default parameters are reasonably good choices, and there is unlikely to be significant improvements from optimisation.</li>

      </br><h2 class="text-left"><small>Next Steps</small></h2>
      Several options exist for further model improvement which we would like to apply in future competitions:
      <div><br><ul style="list-style-type:disc">
        <li>Combining features to look at cross-terms. Due to the already large feature space, care must be taken!</li>
        <li>Trying non-linear models, again due to the large dataset this has computational issues.</li>
        <li>Ensemble learning methods, this appears to be a very common approach to improve models in Kaggle competitions.</li>
      </ul></div>
      In conclusion Kaggle appears to be an excellent place for accessing datasets and practise applying machine learning algorithms. For ourselves though, as we look to understand how to tailor and improve upon algorithms, perhaps Kaggle's best feature is its community and experts who are sharing their code and approaches as we learn to solve industry problems together.

      <br><br>
    </div>
  </div>
</div>

<script type="text/javascript">
  //Ensure body always stays below the nav-bar
  $(window).resize(function () { 
    $('body').css('padding-top', parseInt($('#navLeft').css("height")));
  });

  $(window).load(function () { 
      $('body').css('padding-top', parseInt($('#navLeft').css("height")));        
  });
</script>

</body>
</html>